{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdaX080IxI6Clxz3rFknUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikas123456778009/PubMed/blob/main/pubmeds_project_Vikas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqh7U2nvny9Q",
        "outputId": "10df3a87-6633-4987-d969-6ad274621b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Requirement already satisfied: requests_html in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.32.3)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.0.0)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from requests_html) (1.5.1)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from requests_html) (1.20.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from requests_html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.2.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (2024.7.4)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (8.4.0)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (11.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.66.5)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.26.19)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests_html) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (4.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests_html) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests_html) (2.6)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.37.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=483b240a89696f0b2f106a7ded4ae3b990b7c41b7e3ad777a8447d5dd7a68ae4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ],
      "source": [
        "%pip install biopython --upgrade\n",
        "%pip install requests_html\n",
        "%pip install pdfplumber\n",
        "%pip install streamlit\n",
        "%pip install fpdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from Bio import Entrez\n",
        "from requests_html import HTMLSession\n",
        "import requests\n",
        "from requests.exceptions import ConnectionError\n",
        "import pdfplumber\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"PMC Searcher and Summarizer\")\n",
        "\n",
        "# Function to search PMC\n",
        "def search_pmc(search_term, max_results):\n",
        "    Entrez.email = \"vikasdewangan218@gmail.com\"  # Set your email for NCBI API access\n",
        "\n",
        "    # Use the esearch function to search for articles in PMC\n",
        "    handle = Entrez.esearch(db=\"pmc\", term=search_term, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    return record[\"IdList\"]\n",
        "\n",
        "# Convert PDF to text\n",
        "def pdf_to_text(pdf_file):\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            page_text = re.sub(r'\\<\\/?[img|IMG|Image]\\>|\\[[A-Za-z]+\\]', '', page_text)\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Load the Pretrained Summarization Model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarize the Text Data\n",
        "def summarize(text, min_words=100, max_words=150):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    summary_words = summary.split()\n",
        "    if len(summary_words) < min_words:\n",
        "        return summary\n",
        "    elif len(summary_words) > max_words:\n",
        "        return ' '.join(summary_words[:max_words])\n",
        "    else:\n",
        "        return summary\n",
        "\n",
        "def main():\n",
        "    search_term_key = \"search_term_input\"\n",
        "    max_results_key = \"max_results_input\"\n",
        "\n",
        "    # Input fields\n",
        "    search_term = st.text_input(\"Enter the search term for PMC: \", key=search_term_key)\n",
        "    max_results = st.number_input(\"Enter the maximum number of results to fetch: \", min_value=1, max_value=10, key=max_results_key)\n",
        "\n",
        "    if search_term and max_results:\n",
        "        st.write(f\"Searching for '{search_term}' with a maximum of {max_results} results.\")\n",
        "        pmc_ids = search_pmc(search_term, max_results)\n",
        "\n",
        "        pdf_dir = \"/content\"\n",
        "        summary_dir = \"/content/summary\"\n",
        "        combined_summary_file = \"combined_summaries.txt\"\n",
        "\n",
        "        s = HTMLSession()\n",
        "        combined_summaries = []\n",
        "\n",
        "        for pmc in pmc_ids:\n",
        "            try:\n",
        "                pmcid = pmc.strip()\n",
        "                base_url = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC'\n",
        "                r = s.get(base_url + pmcid + '/', timeout=3)\n",
        "                pdf_url = 'https://www.ncbi.nlm.nih.gov/' + r.html.find('a.int-view', first=True).attrs['href']\n",
        "                r = s.get(pdf_url, stream=True)\n",
        "                pdf_path = os.path.join(pdf_dir, pmcid + '.pdf')\n",
        "                with open(pdf_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=1024):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                st.success(f\"Downloaded article {pmcid} as PDF.\")\n",
        "\n",
        "                # Process PDF and summarize\n",
        "                text = pdf_to_text(pdf_path)\n",
        "                preprocessed_text = preprocess(text)\n",
        "                summary = summarize(preprocessed_text, min_words=100, max_words=150)\n",
        "\n",
        "                # Add the summary to the combined list with its PubMed ID as the heading\n",
        "                combined_summaries.append(f\"PubMed ID: {pmcid}\\n\\n{summary}\\n\\n\")\n",
        "\n",
        "            except ConnectionError as e:\n",
        "                st.error(f\"Failed to download article {pmcid}.\")\n",
        "\n",
        "        # Save the combined summaries to a text file\n",
        "        with open(combined_summary_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(combined_summaries))\n",
        "\n",
        "        st.success(f\"Combined summaries saved: {combined_summary_file}\")\n",
        "\n",
        "        # Display the combined summaries in Streamlit\n",
        "        with open(combined_summary_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            combined_summary_content = f.read()\n",
        "\n",
        "        st.text_area(\"Combined Summaries\", combined_summary_content, height=400)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "wEp25OpN8ZO6",
        "outputId": "a96b90a8-ff2a-4a65-d6b9-e25e7ce7141a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code don't make pdf but it is fast\n"
      ],
      "metadata": {
        "id": "vVJz-BfI_chQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from Bio import Entrez\n",
        "from requests_html import HTMLSession\n",
        "import requests\n",
        "from requests.exceptions import ConnectionError\n",
        "import pdfplumber\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import re\n",
        "import os\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"PMC Searcher and Summarizer\")\n",
        "\n",
        "# Function to search PMC\n",
        "def search_pmc(search_term, max_results):\n",
        "    Entrez.email = \"vikasdewangan218@gmail.com\"  # Set your email for NCBI API access\n",
        "\n",
        "    handle = Entrez.esearch(db=\"pmc\", term=search_term, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    return record[\"IdList\"]\n",
        "\n",
        "# Convert PDF to text\n",
        "def pdf_to_text(pdf_file):\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            page_text = re.sub(r'\\<\\/?[img|IMG|Image]\\>|\\[[A-Za-z]+\\]', '', page_text)\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Load the Pretrained Summarization Model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarize the Text Data\n",
        "def summarize(text, min_words=150, max_words=200):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    summary_words = summary.split()\n",
        "    if len(summary_words) < min_words:\n",
        "        return summary\n",
        "    elif len(summary_words) > max_words:\n",
        "        return ' '.join(summary_words[:max_words])\n",
        "    else:\n",
        "        return summary\n",
        "\n",
        "# Function to create a PDF from text\n",
        "def create_pdf(text_content, pdf_filename):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, text_content)\n",
        "\n",
        "    pdf.output(pdf_filename)\n",
        "\n",
        "def main():\n",
        "    search_term_key = \"search_term_input\"\n",
        "    max_results_key = \"max_results_input\"\n",
        "\n",
        "    # Input fields\n",
        "    search_term = st.text_input(\"Enter the search term for PMC: \", key=search_term_key)\n",
        "    max_results = st.number_input(\"Enter the maximum number of results to fetch: \", min_value=1, max_value=10, key=max_results_key)\n",
        "\n",
        "    if search_term and max_results:\n",
        "        st.write(f\"Searching for '{search_term}' with a maximum of {max_results} results.\")\n",
        "        pmc_ids = search_pmc(search_term, max_results)\n",
        "\n",
        "        pdf_dir = \"/content\"\n",
        "        summary_dir = \"/content/summary\"\n",
        "        combined_summary_file = \"combined_summaries.txt\"\n",
        "\n",
        "        s = HTMLSession()\n",
        "        combined_summaries = []\n",
        "\n",
        "        for pmc in pmc_ids:\n",
        "            try:\n",
        "                pmcid = pmc.strip()\n",
        "                base_url = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC'\n",
        "                r = s.get(base_url + pmcid + '/', timeout=3)\n",
        "                pdf_url = 'https://www.ncbi.nlm.nih.gov/' + r.html.find('a.int-view', first=True).attrs['href']\n",
        "                r = s.get(pdf_url, stream=True)\n",
        "                pdf_path = os.path.join(pdf_dir, pmcid + '.pdf')\n",
        "                with open(pdf_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=1024):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                st.success(f\"Downloaded article {pmcid} as PDF.\")\n",
        "\n",
        "                # Process PDF and summarize\n",
        "                text = pdf_to_text(pdf_path)\n",
        "                preprocessed_text = preprocess(text)\n",
        "                summary = summarize(preprocessed_text, min_words=100, max_words=150)\n",
        "\n",
        "                # Add the summary to the combined list with its PubMed ID as the heading\n",
        "                combined_summaries.append(f\"PubMed ID: {pmcid}\\n\\n{summary}\\n\\n\")\n",
        "\n",
        "                # Provide a download button for the original PDF\n",
        "                with open(pdf_path, \"rb\") as pdf_file:\n",
        "                    st.download_button(\n",
        "                        label=f\"Download original PDF for {pmcid}\",\n",
        "                        data=pdf_file,\n",
        "                        file_name=f\"{pmcid}.pdf\",\n",
        "                        mime=\"application/pdf\"\n",
        "                    )\n",
        "\n",
        "            except ConnectionError as e:\n",
        "                st.error(f\"Failed to download article {pmcid}.\")\n",
        "\n",
        "        # Save the combined summaries to a text file\n",
        "        with open(combined_summary_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(combined_summaries))\n",
        "\n",
        "        st.success(f\"Combined summaries saved: {combined_summary_file}\")\n",
        "\n",
        "        # Display the combined summaries in Streamlit\n",
        "        with open(combined_summary_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            combined_summary_content = f.read()\n",
        "\n",
        "        st.text_area(\"Combined Summaries\", combined_summary_content, height=400)\n",
        "\n",
        "        # Create and provide a PDF download option for the combined summaries\n",
        "        pdf_filename = \"combined_summaries.pdf\"\n",
        "        create_pdf(combined_summary_content, pdf_filename)\n",
        "\n",
        "        with open(pdf_filename, \"rb\") as pdf_file:\n",
        "            st.download_button(\n",
        "                label=\"Download Summaries PDF\",\n",
        "                data=pdf_file,\n",
        "                file_name=pdf_filename,\n",
        "                mime=\"application/pdf\"\n",
        "            )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGPA0omrn3Op",
        "outputId": "5f0000e6-cbb8-40da-cc95-cfc49ba4a3d8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This make pdf but it is slow\n"
      ],
      "metadata": {
        "id": "UhO89WKf_jFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "9P0XoKyar-rw",
        "outputId": "b615db5d-89d6-4135-88b6-c3c1ae748d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.188.228.136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "9W9WPgcVthW6",
        "outputId": "fd1309c4-1ba9-4801-91a9-7d2790676dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.188.228.136:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://tall-carrots-relax.loca.lt\n"
          ]
        }
      ]
    }
  ]
}
